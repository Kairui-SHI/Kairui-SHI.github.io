<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kairui Shi's Homepage</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/love.jpg">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kairui Shi
                </p>
                <p> I'm a senior undergraduate student at <strong>Wuhan University</strong>, majoring in <strong>Photogrammetry and Remote Sensing</strong>.                 Expected to graduate in <strong>July 2025</strong>.
                </p>
                <p>I am currently a visiting student at <a href="https://ai4ce.github.io/">AI4CE Lab@NYU</a> led by <a href="https://engineering.nyu.edu/faculty/chen-feng"> Prof. Chen Feng</a>.
                </p>
                <p>
                  In my free time I enjoy playing with MCU/FPGA boards. I am also a fan of clothing/jewelry design and video games. 
                </p>
                <p>
                </p>
                <p style="text-align:center">
                  <a href="mailto:ks8018@nyu.edu">Email</a> &nbsp/&nbsp
                  <a href="data/Kairui-Resume.pdf">CV</a> &nbsp/&nbsp
                  <!-- <a href="https://scholar.google.com/citations?user=0jVr_XwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                  <a href="https://github.com/Kairui-SHI">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Kairui Shi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kairui Shi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                  I am interested in <b>multi-modal robotic perception (tactile, visual, biosignal, etc.)</b>, and perception algorithm that makes robots more viable in real life with <b>better power efficiency, execution speed</b> and so on.
                  <br>
                  <br>
                  I use tools like <b>deep learning, neuromorphic computing, model predictive control </b>and etc..
                  <br>
                  <br>But <b>most importantly</b>, at the fundamental level, I am interested in research that can enhance human experiences instead of replacing humans.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr onmouseout="luwa_stop()" onmouseover="luwa_start()" >
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='luwa_image'>
                    <img src='images/luwa/luwa2.jpg' width="160"></div>
                  <img src='images/luwa/luwa1.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function luwa_start() {
                    document.getElementById('luwa_image').style.opacity = "1";
                  }
  
                  function luwa_stop() {
                    document.getElementById('luwa_image').style.opacity = "0";
                  }
                  luwa_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=40NyBeZQ0G&noteId=WurF6gCiKJ">
                  <papertitle>LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/jing-zhang-4b7163285/"> Jing Zhang*</a>,
                <strong> Irving Fang*</strong>, 
                <a href="">Hao Wu</a>, 
                <a href="https://www.linkedin.com/in/akshat-kaushik/">Akshat Kaushik</a>, 
                <a href="https://as.nyu.edu/departments/anthropology/people/graduate-students/doctoral-students/alice-rodriguez.html">Alice Rodriguez</a>, 
                <a href="https://www.linkedin.com/in/hanwen-zhao-2523a4104/">Hanwen Zhao</a>, 
                <a href="https://juexzz.github.io/">Juexiao Zhang</a>, 
                <a href="">Zhuo Zheng</a>, 
                <a href="https://wp.nyu.edu/faculty-iovita/">Radu Iovita</a>,
                <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a>
                (* for equal contribution)
                <br>
                <em>CVPR</em>, 2024. <strong> Highlight (11.9% of 2719 accepted papers)</strong>
                <br>
                <a href="https://ai4ce.github.io/LUWA/">project page</a>
                /
                <a href="https://arxiv.org/abs/2403.13171">arXiv</a>
                
                <p></p>
                <p>Paleoanthropology meets cutting-edge computer vision! <br> We create the first Lithic Use-Wear Analysis (LUWA) dataset and challenge Large Vision Model and Large Language and Vision Model with it.</p>
              </td>
            </tr>
            
            <tr onmouseout="egopat3d_v2_stop()" onmouseover="egopat3d_v2_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" ><video  width=160 height=130 id='egopat3d_v2_video' muted autoplay loop>
                  <source src="images/egopat3d_v2/egopat3d_after.mp4" type="video/mp4">
                  </video></div>
                  <img src='images/egopat3d_v2/egopat3d_v2_before.png' width="160" height="130" id="egopat3d_v2_img">
                </div>
                <script type="text/javascript">
                  function egopat3d_v2_start() {
                    // when mouse is over the image, make the video visible and the image invisible
                    document.getElementById('egopat3d_v2_video').style.opacity = "1";
                    document.getElementById('egopat3d_v2_img').style.opacity = "0";
                  }
  
                  function egopat3d_v2_stop() {
                    document.getElementById('egopat3d_v2_video').style.opacity = "0";
                    document.getElementById('egopat3d_v2_img').style.opacity = "1";
                  }
                  egopat3d_v2_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.05046">
                  <papertitle>EgoPAT3Dv2: Predicting 3D Action Target from 2D Egocentric Vision for Human-Robot Interaction</papertitle>
                </a>
                <br>
                <strong> Irving Fang*</strong>,
                <a href="https://github.com/yuzhongchen/">Yuzhong Chen*</a>
                <a href="https:">Yifan Wang*</a>
                <a href="https:">Jianghan Zhang&dagger;</a>,
                <a href="https:">Qiushi Zhang&dagger;</a>,
                <a href="https:">Jiali Xu&dagger;</a>,
                <a href="https:">Xibo He</a>,
                <a href="https:">Weibo Gao</a>,
                <a href="https:">Hao Su</a>,
                <a href="https://yimingli-page.github.io/">Yiming Li</a>,
                <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a>
                (*, &dagger;for equal contribution)
                <br>
                <em>ICRA 2024</em>
                <br>
                <a href="https://ai4ce.github.io/EgoPAT3Dv2/">project page</a>
                /
                <a href="https://arxiv.org/abs/2303.09192">arXiv</a>
                
                <p></p>
                <p>Human-robot interaction for a potentially AR world?</p>
              </td>
            </tr> 
  
            <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/atm/atm_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/atm/atm_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function dreamfusion_start() {
                    document.getElementById('dreamfusion_image').style.opacity = "1";
                  }
  
                  function dreamfusion_stop() {
                    document.getElementById('dreamfusion_image').style.opacity = "0";
                  }
                  dreamfusion_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.roboticsproceedings.org/rss19/p099.html">
                  <papertitle>DeepExplorer: Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space</papertitle>
                </a>
                <br>
                <a href="https://yuhanghe01.github.io/"> Yuhang He*</a>,
                <strong> Irving Fang*</strong>,
                <a href="https://yimingli-page.github.io/">Yiming Li</a>,
                <a href="https://www.linkedin.com/in/rushishah2210/">Rushi Bhavesh Shah</a>,
                <a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng</a>
  
                (* for equal contribution)
                <br>
                <em>RSS 2023</em>
                <br>
                <a href="https://ai4ce.github.io/DeepExplorer/">project page</a>
                /
                <a href="https://arxiv.org/abs/2303.09192">arXiv</a>
                
                <p></p>
                <p>A simple and effective framework for efficient and lightweight active visual exploration with only RGB images as input</p>
              </td>
            </tr> 
  
            <tr onmouseout="mira_stop()" onmouseover="mira_start()" >
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mira_image'>
                    <img src='images/squishy/squishy_after.png' width="160"></div>
                  <img src='images/squishy/squishy_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mira_start() {
                    document.getElementById('mira_image').style.opacity = "1";
                  }
  
                  function mira_stop() {
                    document.getElementById('mira_image').style.opacity = "0";
                  }
                  mira_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://asmedigitalcollection.asme.org/IMECE/proceedings-abstract/IMECE2021/85697/V013T14A027/1133300">
                  <papertitle>Dynamic Placement of Rapidly Deployable Mobile Sensor Robots Using Machine Learning and Expected Value of Information</papertitle>
                </a>
                <br>
                <a href="https://me.berkeley.edu/people/alice-m-agogino/"> Alice Agogino</a>,
                <a href="https://www.linkedin.com/in/hae-young-jang-7073071b5/">Hae Young Jang</a>,
                <a href="https://haas.berkeley.edu/faculty/vivek-rao/">Vivek Rao</a>, 
                <a href="https://www.linkedin.com/in/ritikbatra/">Ritik Batra</a>, 
                <a href="https://www.linkedin.com/in/felicityliao/">Felicity Liao</a>, 
                <a href= "https://www.linkedin.com/in/rohansood10/">Rohan Sood</a>, 
                <strong> Irving Fang</strong>, 
                <a href="http://rlily.hu/">R Lily Hu</a>, 
                <a href="https://www.linkedin.com/in/emerson-shoichet-bartus/">Emerson Shoichet-Bartus</a>, 
                <a href="https://www.linkedin.com/in/johnmatranga/">John Matranga</a>
                (Authors ordered by department affiliation, not contribution)
                <br>
                <em>ASME IMECE</em>, 2021
                <br>
                <a href="https://github.com/BerkeleyExpertSystemTechnologiesLab/EVSIvsLSTM">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.07552">arXiv</a>
                
                <p></p>
                <p>A framework for optimizing the deployment of emergency sensors using Long Short-Term Memory (LSTM) Neural Network and Expected Value of Information (EVI)</p>
              </td>
            </tr> 
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Personal Projects</heading>
                <p>
                  Please visit <a href="https://github.com/IrvingF7/my_project_list">this repo</a>. It contains pointers to some personal projects ranging from robotics to a RISC-V CPU implemented on a Xilinx FPGA board.
                </p>
              </td>
            </tr>
          </tbody></table>
  
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
                Teaching Aide, ROB-UY 3203 Robot Vision, Spring 2023 <br>
                Teaching Aide, ROB-GY 6203 Robot Perception, Fall 2022 <br>
                Teaching Aide, ROB-UY 3203 Robot Vision, Spring 2022 <br>
              </p>
            </td>
          </tr>
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <p>
                Reviewer, ICRA2024 <br>
                Reviewer, DARS2024 <br>
              </p>
            </td>
          </tr>
          </tbody></table>
  
  
  
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  The website is based on Dr. Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>,
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
  
  </html>
